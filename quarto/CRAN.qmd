---
title: "CRAN"
format: 
  html: 
    page-layout: full
    toc: true
    toc-location: left
    toc-depth: 5
editor: visual
---

```{r Loading Data, echo=FALSE,warning=FALSE,message=FALSE}
source('../scripts/functions.R')
df = load_data('rpkg_20190129.csv')
df0 = df-1

```

# Zeta Model

We first consider modelling our data using the zeta model which is a power law model that has p.m.f:

$$
f(x) = \zeta(\alpha+1)^{-1}x^{-(\alpha+1)}, \qquad x=1,2,3,\ldots
$$

and has survival function:

$$
S(x) = 1-\zeta(\alpha+1)^{-1}\sum_{k=1}^x k^{-(\alpha+1)}, \qquad x=1,2,3,\ldots 
$$

If we have a vector of data $x = (x_1,x_2,\ldots,x_N)^T$ , then the likelihood function and log-likelihood functions are:

$$
L(x) = \zeta(\alpha+1)^{-N}\prod_{i=1}^N x_i^{-(\alpha+1)}
$$

and,

$$
\ell(x) = -N\log\zeta(\alpha+1) - (\alpha+1)\sum_{i=1}^N\log x_i
$$

We now aim to fit this model using a Metropolis-Hastings algorithm, which means we need to put a prior on to the value of $\alpha$. Since its value is restricted to being positive we choose to use a Gamma prior, that is:

$$
\alpha \sim Ga(\gamma,\delta)
$$

with the prior density function being:

$$
\pi(\alpha) \propto \alpha^{\gamma-1}e^{-\delta\alpha}
$$

Now, we can calculate the posterior distribution up to a constant of proportionality:

$$
\pi(\alpha|x) \propto\pi(\alpha)L(x)
$$

But, it will be better to use the log of the posterior distribution to mitigate any computational issues that may arise.

$$
\log\pi(\alpha|x) = \log A + (\gamma-1)\log\alpha-N\log\zeta(\alpha+1)-\delta\alpha-(\alpha+1)\sum_{i=1}^N\log x_i
$$

With this posterior we can fit the model to both sets of our data.

## Plots of Zeta Model Fit

### Depends Data

```{r Zeta Depends,echo=FALSE,message=FALSE,warning=FALSE}
zeta.depends.out = zeta.mcmc(1e4,df$depends,2,c(4,4))
```

### Imports Data

```{r Zeta Imports ,echo=FALSE,message=FALSE,warning=FALSE}
zeta.imports.out = zeta.mcmc(1e4,df$imports,1.5,c(4,4))
```

### Depends Data (without zeros)

```{r Zeta Depends no zeros ,echo=FALSE,message=FALSE,warning=FALSE}
zeta.depends0.out = zeta.mcmc(1e4,df0$depends[df0$depends>0],1,c(4,4))

```

### Imports Data (without zeros)

```{r  Zeta Imports no zeros ,echo=FALSE,message=FALSE,warning=FALSE}
zeta.imports0.out = zeta.mcmc(1e4,df0$imports[df0$imports>0],0.6,c(4,4))
```

# Zeta-IGPD Model

Since the zeta model seems to not fit very well, we now aim to fit a new model to the data that may fit better. This model model the data as before below a certain threshold but above it the data is modeled using the integer-valued generalised Pareto distribution. The p.m.f of this model is

$$
f(x) =
\begin{cases}
\zeta(\alpha+1)^{-1}x^{-(\alpha+1)}&, x\leq u\\
\left[1-\zeta(\alpha+1)^{-1}\sum_{k=1}^u k^{-(\alpha+1)}\right]\left\{\left(1+\frac{\xi(x-1)}{\sigma_u}\right)_+^{-1/\xi} - \left(1+\frac{\xi x}{\sigma_u}\right)_+^{-1/\xi}\right\}&,x>u
\end{cases}
$$

And the cumulative mass function is given by:

$$
F(x) = \begin{cases}
\zeta(\alpha+1)^{-1}\sum_{k=1}^xk^{-(\alpha+1)}&,x\leq u\\
\zeta(\alpha+1)^{-1}\sum_{k=1}^uk^{-(\alpha+1)} + \left[1-\zeta(\alpha+1)^{-1}\sum_{k=1}^u k^{-(\alpha+1)}\right]\sum_{k=u+1}^x\left\{\left(1+\frac{\xi(k-1)}{\sigma_u}\right)_+^{-1/\xi} - \left(1+\frac{\xi k}{\sigma_u}\right)_+^{-1/\xi}\right\}&,x>u
\end{cases}
$$

This can be simplified to:

$$
F(x) = \begin{cases}
\zeta(\alpha+1)^{-1}\sum_{k=1}^xk^{-(\alpha+1)}&,x\leq u\\
\zeta(\alpha+1)^{-1}\sum_{k=1}^uk^{-(\alpha+1)} + \left[1-\zeta(\alpha+1)^{-1}\sum_{k=1}^u k^{-(\alpha+1)}\right]\left\{1 - \left(1+\frac{\xi k}{\sigma_u}\right)_+^{-1/\xi}\right\}&,x>u
\end{cases}
$$

Which makes the survival function of this model:

$$
S(x) = 
\begin{cases}
1-\zeta(\alpha+1)^{-1}\sum_{k=1}^x k^{-(\alpha+1)}&,x\leq u\\
\left[1-\zeta(\alpha+1)^{-1}\sum_{k=1}^u k^{-(\alpha+1)}\right]\left(1+\frac{\xi x}{\sigma_u}\right)_+^{-1/\xi}&,x>u
\end{cases}
$$

If we have a vector of data $x = (x_1,x_2,\ldots x_N)^T$ then the likelihood and log-likelihood are:

$$
L(x) = \zeta(\alpha+1)^{-n}C_{\alpha,u}^{N-n}\prod_{i:x_i\leq u}x_i^{-(\alpha+1)}\prod_{i:x_i>u}\left\{\left(1+\frac{\xi(x_i-1)}{\sigma_u}\right)_+^{-1/\xi} - \left(1+\frac{\xi x_i}{\sigma_u}\right)_+^{-1/\xi}\right\}
$$

and,

$$
\ell(x) = -n\log\zeta(\alpha+1)+(N-n)\log C_{\alpha,u} -(\alpha+1)\sum_{i:x_i\leq u}\log x_i + \sum_{i:x_i>u}\log\left\{\left(1+\frac{\xi(x_i-1)}{\sigma_u}\right)_+^{-1/\xi} - \left(1+\frac{\xi x_i}{\sigma_u}\right)_+^{-1/\xi}\right\}
$$

where $C_{\alpha,u} = 1-\zeta(\alpha+1)^{-1}\sum_{k=1}^u k^{-(\alpha+1)}$.

Since we wish to use this to construct a Metropolis-Hastings algorithm we again need some priors for the values of the parameters. The priors we use are listed below.

$$
\alpha\sim Ga(\gamma,\delta),\\
\xi \sim N(0,s^2),\\
\sigma_u \sim Ga(\phi,\psi)
$$

Our joint prior distribution is defined as follows:

```{=tex}
\begin{align}
\pi(\alpha,\xi,\sigma_u) &= \pi(\alpha)\pi(\xi)\pi(\sigma_u)\\
\log\pi(\alpha,\xi\sigma_u) &= \log\pi(\alpha) +\log\pi(\xi) +\log\pi(\sigma_u)
\end{align}
```
And we have that

```{=tex}
\begin{align}
\log\pi(\alpha) &= \gamma\log\delta - \log\Gamma(\gamma) + (\gamma-1)\log\alpha -\delta\alpha\\
\log\pi(\xi) &= -\log s -0.5\log 2\pi - \frac{\xi^2}{2s^2}\\
\log\pi(\sigma_u) &= \phi\log\psi - \log\Gamma(\phi) + (\phi-1)\log\sigma_u - \psi\sigma_u
\end{align}
```
Which we can then use to construct the joint posterior:

```{=tex}
\begin{align}
\pi(\alpha,\xi,\sigma_u|x) &\propto L(x)\pi(\alpha,\xi,\sigma_u)\\
                            &=L(x)\pi(\alpha)\pi(\xi)\pi(\sigma_u)\\
\log\pi(\alpha,\xi,\sigma_u|x) &= \log A + \ell(x) +  \log\pi(\alpha) +\log\pi(\xi) + \log\pi(\sigma_u)\\
&= \log B - n\log\zeta(\alpha+1) - (\alpha + 1)\sum_{i:x_i\leq u}\log x_i + (N-n)\log C_{\alpha,u} + \sum_{i:x_i>u}\log[G(x_i-1) - G(x_i)]\\

&\qquad + (\gamma-1)\log\alpha - \delta\alpha -\frac{\xi^2}{2s^2} + (\phi-1)\log\sigma_u - \psi\sigma_u 
\end{align}
```
We can now use this to fit the model for various thresholds, the thresholds we will be using will be based on the quantiles of our data.

## Method for fitting

In order to fit this model we will be using a Metropolis-Hastings algorithm with an adaptive proposal distribution, the method is fully described below:

Suppose we have the initial state $\theta^{(0)} = (\alpha^{(0)},\xi^{(0)},\sigma_u^{(0)})$ and an initial proposal covariance matrix $\Sigma^{(0)}$. Let $\Omega$ be the ordered set of accepted states that is initially empty. Starting with $t=1$.

1.  

    -   If $|\Omega|<H$ then:

        -   Set $\Sigma^{(t)} = \Sigma^{(0)}$

    -   Otherwise:

        -   Set $\Sigma^{(t)} =\frac{2.38^2}{3}R_m$ , where $R_m$ is the empirical covariance matrix of the last $H$ accepted states.

2.  Propose a new state $\theta^* \sim N(\theta^{(t-1)},\Sigma^{(t)})$

3.  

    -   If $\alpha^*<0$ or $\sigma_u^*<0$ then:

        -   Reject: set $\theta^{(t)} = \theta^{(t-1)}$

        -   Set $t=t+1$ and go to 1.

4.  Calculate $\mathcal{A} = \min(0,\log\pi(\theta^*|x) - \log\pi(\theta^{(t-1)}|x))$

5.  Draw $z\sim U(0,1)$

    -   If $\log z <\mathcal{A}$ then:

        -   Accept: set $\theta^{(t)} = \theta^*$

        -   Add $\theta^{(t)}$ to the end of $\Omega$

    -   Otherwise

        -   Reject: set $\theta^{(t)} = \theta^{(t-1)}$

6.  Set $t=t+1$ and go to 1.

## Plots of fitted model

```{r}
n.iter=4e4
```

### Depends data

#### Quantile 0.95

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$depends,0.95)
mcmc.out=fast.zigpd.mcmc(n.iter,df$depends,u)
```

#### Quantile 0.97

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$depends,0.97)
mcmc.out=fast.zigpd.mcmc(n.iter,df$depends,u)
```

#### Quantile 0.98

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$depends,0.98)
mcmc.out=fast.zigpd.mcmc(n.iter,df$depends,u)
```

#### Quantile 0.99

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$depends,0.99)
mcmc.out=fast.zigpd.mcmc(n.iter,df$depends,u)
```

### Imports data

#### Quantile 0.95

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$imports,0.95)
mcmc.out=fast.zigpd.mcmc(n.iter,df$imports,u)
```

#### Quantile 0.97

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$imports,0.97)
mcmc.out=fast.zigpd.mcmc(n.iter,df$imports,u)
```

#### Quantile 0.98

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$imports,0.98)
mcmc.out=fast.zigpd.mcmc(n.iter,df$imports,u)
```

#### Quantile 0.99

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df$imports,0.99)
mcmc.out=fast.zigpd.mcmc(n.iter,df$imports,u)
```

### Depends Without Zeros

#### Quantile 0.95

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$depends[df0$depends>0],0.95)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$depends[df0$depends>0],u)
```

#### Quantile 0.97

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$depends[df0$depends>0],0.97)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$depends[df0$depends>0],u)
```

#### Quantile 0.98

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$depends[df0$depends>0],0.98)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$depends[df0$depends>0],u)
```

#### Quantile 0.99

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$depends[df0$depends>0],0.99)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$depends[df0$depends>0],u)
```

### Imports Without Zeros

#### Quantile 0.95

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$imports[df0$imports>0],0.95)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$imports[df0$imports>0],u)
```

#### Quantile 0.97

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$imports[df0$imports>0],0.97)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$imports[df0$imports>0],u)

```

#### Quantile 0.98

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$imports[df0$imports>0],0.98)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$imports[df0$imports>0],u)
```

#### Quantile 0.99

```{r,echo=FALSE,message=FALSE,warning=FALSE}
u = quantile(df0$imports[df0$imports>0],0.99)
mcmc.out=fast.zigpd.mcmc(n.iter,df0$imports[df0$imports>0],u)
```

# Notes

## Implementation of zeta function

Below is a plot demonstrating the difference in the values obtained from the two different implementations of the hurwitz zeta function.There appears to be a large difference, with the gsl library being more accurate, evidenced by the fact that we can increase the 'aa' variable in the function from VGAM and obtain values closer to those obtained from gsl.

### Defaults

```{r, echo=FALSE}
AA = 24
zeta_mod = function (x, deriv = 0, shift = 1,aa=12) 
{
    deriv.arg <- deriv
    rm(deriv)
    if (!is.Numeric(deriv.arg, length.arg = 1, integer.valued = TRUE)) 
        stop("'deriv' must be a single non-negative integer")
    if (deriv.arg < 0 || deriv.arg > 2) 
        stop("'deriv' must be 0, 1, or 2")
    if (deriv.arg > 0) 
        return(zeta.specials(Zeta.derivative(x, deriv.arg = deriv.arg, 
            shift = shift), x, deriv.arg, shift))
    if (any(special <- Re(x) <= 1)) {
        ans <- x
        ans[special] <- Inf
        special3 <- Re(x) < 1
        ans[special3] <- NA
        special4 <- (0 < Re(x)) & (Re(x) < 1) & (Im(x) == 0)
        ans[special4] <- Zeta.derivative(x[special4], deriv.arg = deriv.arg, 
            shift = shift)
        special2 <- Re(x) < 0
        if (any(special2)) {
            x2 <- x[special2]
            cx <- 1 - x2
            ans[special2] <- 2^(x2) * pi^(x2 - 1) * sin(pi * 
                x2/2) * gamma(cx) * Recall(cx)
        }
        if (any(!special)) {
            ans[!special] <- Recall(x[!special])
        }
        return(zeta.specials(ans, x, deriv.arg, shift))
    }
    ans <- 0
    for (ii in 0:(aa - 1)) ans <- ans + 1/(shift + ii)^x
    ans <- ans + Zeta.aux(shape = x, aa, shift = shift)
    ans[shift <= 0] <- NaN
    zeta.specials(ans, x, deriv.arg = deriv.arg, shift = shift)
}
```

```{r, echo=FALSE}
library(latex2exp)

k = 1:2000
s = 3

plot(k,gsl::hzeta(s,q=k), log='xy', type='l',lty=1, col=1,ylab=TeX('\\zeta(s,k)'))
lines(k,zeta_mod(s,shift=k), lty=2)

```

### Increasing the value of aa

```{r,echo=FALSE}
aa = 3e2
plot(k,gsl::hzeta(s,q=k), log='xy', type='l',lty=1, col=1,ylab=TeX('\\zeta(s,k)'))
lines(k,zeta_mod(s,shift=k,aa=aa), lty=2)

```
